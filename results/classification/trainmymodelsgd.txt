training: 13195 , validation: 16965
1885
Training and Testing my model
1.2.0
Net(
  (conv1): Conv2d(4, 128, kernel_size=(2, 2), stride=(2, 2))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=262144, out_features=4096, bias=True)
  (fc1_bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=4096, out_features=29, bias=True)
)
12
torch.Size([128, 4, 2, 2])
[1,     1] loss: 0.662, total correct: 2
training: 13195 , validation: 16965
1885
Training and Testing my model
1.2.0
Net(
  (conv1): Conv2d(4, 128, kernel_size=(2, 2), stride=(2, 2))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=262144, out_features=4096, bias=True)
  (fc1_bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=4096, out_features=29, bias=True)
)
12
torch.Size([128, 4, 2, 2])
[1,     1] loss: 0.701, total correct: 3
[2,     1] loss: 0.260, total correct: 21
[3,     1] loss: 0.174, total correct: 37
[4,     1] loss: 0.139, total correct: 36
[5,     1] loss: 0.118, total correct: 43
[6,     1] loss: 0.101, total correct: 54
[7,     1] loss: 0.095, total correct: 53
[8,     1] loss: 0.082, total correct: 56
[9,     1] loss: 0.084, total correct: 55
[10,     1] loss: 0.075, total correct: 62
[11,     1] loss: 0.072, total correct: 57
[12,     1] loss: 0.068, total correct: 57
training: 13195 , validation: 16965
1885
Training and Testing my model
1.2.0
Net(
  (conv1): Conv2d(4, 128, kernel_size=(2, 2), stride=(2, 2))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=262144, out_features=4096, bias=True)
  (fc1_bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=4096, out_features=29, bias=True)
)
12
torch.Size([128, 4, 2, 2])
[1,     1] loss: 0.713, total correct: 1
[2,     1] loss: 0.263, total correct: 27
[3,     1] loss: 0.171, total correct: 44
[4,     1] loss: 0.140, total correct: 39
[5,     1] loss: 0.117, total correct: 43
[6,     1] loss: 0.106, total correct: 45
[7,     1] loss: 0.100, total correct: 46
[8,     1] loss: 0.087, total correct: 53
[9,     1] loss: 0.083, total correct: 51
[10,     1] loss: 0.076, total correct: 62
[11,     1] loss: 0.075, total correct: 56
[12,     1] loss: 0.069, total correct: 61
[13,     1] loss: 0.068, total correct: 57
[14,     1] loss: 0.068, total correct: 57
[15,     1] loss: 0.059, total correct: 63
[16,     1] loss: 0.062, total correct: 61
[17,     1] loss: 0.065, total correct: 61
[18,     1] loss: 0.055, total correct: 63
[19,     1] loss: 0.058, total correct: 62
[20,     1] loss: 0.058, total correct: 61
[21,     1] loss: 0.055, total correct: 60
[22,     1] loss: 0.054, total correct: 63
[23,     1] loss: 0.047, total correct: 62
[24,     1] loss: 0.048, total correct: 64
[25,     1] loss: 0.050, total correct: 64
[26,     1] loss: 0.047, total correct: 64
[27,     1] loss: 0.047, total correct: 64
[28,     1] loss: 0.046, total correct: 63
[29,     1] loss: 0.044, total correct: 64
[30,     1] loss: 0.045, total correct: 62
[31,     1] loss: 0.044, total correct: 64
[32,     1] loss: 0.039, total correct: 63
[33,     1] loss: 0.041, total correct: 63
[34,     1] loss: 0.037, total correct: 64
[35,     1] loss: 0.037, total correct: 63
[36,     1] loss: 0.038, total correct: 63
[37,     1] loss: 0.042, total correct: 64
[38,     1] loss: 0.041, total correct: 64
[39,     1] loss: 0.039, total correct: 63
[40,     1] loss: 0.036, total correct: 64
[41,     1] loss: 0.038, total correct: 64
[42,     1] loss: 0.030, total correct: 64
[43,     1] loss: 0.038, total correct: 64
[44,     1] loss: 0.034, total correct: 64
[45,     1] loss: 0.034, total correct: 64
[46,     1] loss: 0.029, total correct: 64
[47,     1] loss: 0.031, total correct: 64
[48,     1] loss: 0.033, total correct: 64
[49,     1] loss: 0.032, total correct: 64
[50,     1] loss: 0.031, total correct: 64
Training time: 1947m 9s
True label: [14, 5, 17, 3, 17, 5, 15, 3, 9, 5, 9, 17, 5, 3, 9, 5, 14, 9, 3, 16, 9, 14, 14, 2, 9, 16, 15, 9, 2, 5, 3, 14, 15, 3, 17, 5, 4, 14, 14, 2, 9, 4, 4, 9, 14, 2, 2, 2, 5, 5, 2, 2, 2, 13, 14, 5, 16, 3, 9, 2]
Pred_label: [9, 2, 2, 9, 2, 2, 2, 9, 9, 2, 9, 2, 2, 9, 9, 2, 9, 9, 9, 2, 2, 9, 9, 2, 2, 2, 2, 9, 2, 2, 9, 9, 2, 9, 2, 2, 9, 9, 9, 2, 9, 9, 9, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 9, 2, 2]
real loss [0.0]
real accuracy: [0.2833333333333333]
real f1score: [0.2833333333333333]
real truth: [tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 1.,  ..., 0., 0., 0.]], dtype=torch.float64)]
real pred: [tensor([[ -5.2967,  -4.8969,  -3.1050,  ...,  -5.8974,  -6.9983,  -5.8475],
        [ -1.3138,  -6.4112,   2.1831,  ...,  -6.7856,  -6.6840,  -6.2471],
        [ -2.3251,  -5.8433,   2.4630,  ...,  -7.0501,  -6.6306,  -6.6751],
        ...,
        [ -6.4411,  -5.2860,  -7.4808,  ...,  -9.2128, -12.5556, -10.3472],
        [ -1.7949,  -4.5786,   0.2635,  ...,  -6.4058,  -6.4770,  -6.2607],
        [ -0.3122,  -6.5063,   2.4684,  ...,  -6.7547,  -6.4551,  -6.2497]],
       dtype=torch.float64)]
training loss [0.421475616662688, 0.21226445124601767, 0.15472234174343402, 0.1279290545660299, 0.11201729529957238, 0.10126876606178092, 0.09327312941786851, 0.08703911435530602, 0.08192161469662332, 0.07761420841830562, 0.073992360129503, 0.07089372328268569, 0.06805047858631727, 0.06550138864420765, 0.06323154445210805, 0.06117663141694624, 0.05929029812462466, 0.057545286590070514, 0.05600885749287955, 0.05440162991404682, 0.05300086223451608, 0.05168418764240138, 0.050467639639640016, 0.04933526289939432, 0.048078409320886244, 0.04709686218165395, 0.046055728704973514, 0.04518069311500742, 0.04420070606219318, 0.043298575463065904, 0.04243706215094338, 0.04160709625021805, 0.040805043180984664, 0.040059022436197605, 0.039282161050692835, 0.03865290907005592, 0.03789011992432567, 0.03723583181893718, 0.036649509967860545, 0.03604094024000031, 0.035455139111086706, 0.034971101712211454, 0.03429943889921381, 0.033711349864052276, 0.03315252984832882, 0.03267809190984997, 0.03218236297499663, 0.031587357264410004, 0.03113830786851586, 0.03062688842182474]
validation loss: [0.2665396859678757, 0.17730276669007192, 0.141943441728928, 0.12312542045914793, 0.11116935283295251, 0.1022860648645887, 0.09597126761678523, 0.09095141583978453, 0.08663036951053035, 0.08358477961955811, 0.08061583883586024, 0.07799435154254822, 0.07570869152643507, 0.07377768102736415, 0.07207975440617674, 0.07039014755558691, 0.0692852726330115, 0.06763776104651813, 0.06637899948674376, 0.06535147696955511, 0.0643312227681575, 0.06347698241743109, 0.062366885645997676, 0.06135043283446107, 0.060768283596831354, 0.05997355972722798, 0.059123607939551966, 0.05856136904955709, 0.05812676530486239, 0.057344215426769075, 0.056831513087418906, 0.056036501420982356, 0.05580161007374589, 0.05530890038173098, 0.0547986040498408, 0.05424131353741708, 0.0538949332161773, 0.053591597327923686, 0.053168140038657226, 0.0526508915904927, 0.052395723037236525, 0.051845035408968676, 0.05173212256619573, 0.05134832694079996, 0.05101968540259759, 0.0505750692330264, 0.050238704611845174, 0.050137355267843985, 0.049712844758914895, 0.04948072549347098]
testing loss: [0.04963974043338513]
training accuracy: [0.2156119742326639, 0.45782493368700267, 0.578931413414172, 0.6512315270935961, 0.7077680939749905, 0.751496779082986, 0.7859795377036757, 0.812807881773399, 0.8412277377794619, 0.8591890867752937, 0.8797271693823417, 0.8913224706328154, 0.906176582038651, 0.9208791208791208, 0.9277756726032588, 0.9383857521788557, 0.9454338764683592, 0.9512694202349374, 0.9577870405456612, 0.9627889352027283, 0.9657446002273589, 0.9694581280788177, 0.9735505873436908, 0.9762788935202729, 0.9794619173929519, 0.9813565744600228, 0.9832512315270936, 0.9847669571807502, 0.9860553239863584, 0.9878741947707464, 0.9893899204244032, 0.9902993558165972, 0.99037514209928, 0.9923455854490337, 0.9928760894278136, 0.9926487305797651, 0.9930276619931793, 0.994998105342933, 0.9944676013641531, 0.9953770367563471, 0.9953770367563471, 0.9958317544524441, 0.996362258431224, 0.9965138309965896, 0.996362258431224, 0.9971959075407352, 0.9969685486926866, 0.9978021978021978, 0.9978021978021978, 0.997726411519515]
validation accuracy: [0.35251989389920424, 0.4673740053050398, 0.5246684350132625, 0.5631299734748011, 0.596816976127321, 0.6095490716180372, 0.6238726790450928, 0.640318302387268, 0.6453580901856764, 0.6631299734748011, 0.6774535809018568, 0.6803713527851459, 0.6946949602122016, 0.6978779840848807, 0.7031830238726791, 0.7095490716180372, 0.7206896551724138, 0.7244031830238726, 0.7289124668435013, 0.7344827586206897, 0.7395225464190981, 0.7413793103448276, 0.7445623342175066, 0.7490716180371353, 0.7522546419098143, 0.7588859416445624, 0.753315649867374, 0.7612732095490716, 0.7636604774535809, 0.7694960212201591, 0.7697612732095491, 0.7647214854111406, 0.7748010610079575, 0.7790450928381963, 0.773474801061008, 0.7782493368700265, 0.7824933687002652, 0.7787798408488064, 0.7846153846153846, 0.7877984084880637, 0.7904509283819628, 0.7907161803713528, 0.793368700265252, 0.7904509283819628, 0.7909814323607427, 0.793633952254642, 0.7981432360742705, 0.7968169761273209, 0.7984084880636605, 0.7992042440318302]
testing accuracy: [0.7989389920424403]
training fbeta: [0.21743773467911398, 0.4600158462227428, 0.5807571738606221, 0.6537875917186261, 0.7114196148678907, 0.7547831478865962, 0.7885356023287057, 0.8157290984877191, 0.844148954493782, 0.8621103034896138, 0.8822832340073719, 0.8942436873471356, 0.9090977987529711, 0.924530641772021, 0.931062041406869, 0.9420372730717558, 0.9494505494505494, 0.9549209411278377, 0.9618037135278514, 0.9664404560956285, 0.9693961211202591, 0.9734748010610079, 0.977567260325881, 0.9802955665024631, 0.9834785903751421, 0.985373247442213, 0.9869027524199938, 0.9884184780736505, 0.9900719969685486, 0.9915257156636467, 0.9934065934065934, 0.9943160287987874, 0.9943918150814702, 0.996362258431224, 0.9968927624100038, 0.9966654035619553, 0.9966791828860794, 0.9990147783251232, 0.9984842743463433, 0.9993937097385374, 0.9990285576492473, 0.9998484274346343, 1.0003789314134142, 1.0001653518894897, 1.0003789314134142, 1.0012125805229253, 1.000985221674877, 1.001818870784388, 1.001818870784388, 1.0017430845017052]
validation fbeta: [0.3529589316747462, 0.4680874416902954, 0.5256013902862892, 0.5640903686087991, 0.5977499314003475, 0.6105917863349493, 0.6248056343181194, 0.6413884569651513, 0.6463733650416171, 0.6642550077746272, 0.6786334949236257, 0.681194548614287, 0.6956827952071709, 0.6990304582456782, 0.7041434190066771, 0.710646666056892, 0.7218146894722399, 0.7256928564895271, 0.7299826214213847, 0.7356626726424585, 0.7406475807189243, 0.7425317845056252, 0.7456050489344187, 0.7504984908076465, 0.753407116070612, 0.7602030549711882, 0.7544681240281716, 0.7625628830147261, 0.7648403914753499, 0.7707308149638709, 0.7709960669532607, 0.7659562791548523, 0.7760358547516693, 0.7802798865819079, 0.7744626360559773, 0.7795664501966524, 0.7835909631391201, 0.7801243940364035, 0.7859599378029818, 0.7890606420927467, 0.7916857221256746, 0.7919784139760359, 0.7946309338699351, 0.7917406018476173, 0.7920241470776548, 0.794896185859325, 0.7993780298179822, 0.7980243300100613, 0.7997256013902864, 0.8003841580535992]
testing fbeta: [0.8136650507637427]
